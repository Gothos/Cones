{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVZ25b4dIvl_",
        "outputId": "67b543cd-5eee-429f-e0b6-2d85d202d6ba"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLBauUHREqAH"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List, Optional, Union\n",
        "import gc\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import accelerate\n",
        "import diffusers\n",
        "import transformers\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers.models.unet_2d_blocks import CrossAttnDownBlock2D,CrossAttnUpBlock2D\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DItkbyVZDCTY"
      },
      "outputs": [],
      "source": [
        "class DreamBoothDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
        "    It pre-processes the images and the tokenizes prompts.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        concepts_list,\n",
        "        tokenizer,\n",
        "        with_prior_preservation=True,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "        num_class_images=None,\n",
        "        hflip=False\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "        self.with_prior_preservation = with_prior_preservation\n",
        "\n",
        "        self.instance_images_path = []\n",
        "        self.class_images_path = []\n",
        "\n",
        "        for concept in concepts_list:\n",
        "            inst_img_path = [\n",
        "                (x, concept[\"instance_prompt\"])\n",
        "                for x in Path(concept[\"instance_data_dir\"]).iterdir()\n",
        "                if x.is_file() and not str(x).endswith(\".txt\")\n",
        "            ]\n",
        "            self.instance_images_path.extend(inst_img_path)\n",
        "\n",
        "            if with_prior_preservation:\n",
        "                class_img_path = [(x, concept[\"class_prompt\"]) for x in Path(concept[\"class_data_dir\"]).iterdir() if x.is_file()]\n",
        "                self.class_images_path.extend(class_img_path[:num_class_images])\n",
        "\n",
        "        random.shuffle(self.instance_images_path)\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.num_class_images = len(self.class_images_path)\n",
        "        self._length = max(self.num_class_images, self.num_instance_images)\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomHorizontalFlip(0.5 * hflip),\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_path, instance_prompt = self.instance_images_path[index % self.num_instance_images]\n",
        "\n",
        "\n",
        "        instance_image = Image.open(instance_path)\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            instance_prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.with_prior_preservation:\n",
        "            class_path, class_prompt = self.class_images_path[index % self.num_class_images]\n",
        "            class_image = Image.open(class_path)\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                class_prompt,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "\n",
        "        return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9-mWAxcGypS"
      },
      "outputs": [],
      "source": [
        "# The below code uses some code directly or modified from the Huggingface diffusers library.\n",
        "\n",
        "class image_pipe(DiffusionPipeline):\n",
        "  def __init__(self, model_link, device=torch.device('cpu'), dtype=torch.float16 ):\n",
        "    super().__init__()\n",
        "    self.unet=diffusers.UNet2DConditionModel.from_pretrained(model_link, torch_dtype=dtype, subfolder='unet').to(device).train()\n",
        "    self.vae=diffusers.AutoencoderKL.from_pretrained(model_link, torch_dtype=dtype, subfolder='vae').to(device).train()\n",
        "    self.scheduler=diffusers.LMSDiscreteScheduler.from_pretrained(model_link, torch_dtype=dtype, subfolder='scheduler')\n",
        "    self.text_encoder=transformers.CLIPTextModel.from_pretrained(model_link,torch_dtype=dtype,subfolder='text_encoder').to(device).train()\n",
        "    self.tokenizer=transformers.CLIPTokenizer.from_pretrained(model_link,torch_dtype=dtype,subfolder='tokenizer')\n",
        "    self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n",
        "\n",
        "\n",
        "\n",
        "  def _execution_device(self):\n",
        "    return self.unet.device\n",
        "  \n",
        "\n",
        "  @staticmethod\n",
        "  def numpy_to_pil(images):\n",
        "    if images.ndim == 3:\n",
        "      images = images[None, ...]\n",
        "    images = (images * 255).round().astype(\"uint8\")\n",
        "    if images.shape[-1] == 1:\n",
        "      pil_images = [Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n",
        "    else:\n",
        "      pil_images = [Image.fromarray(image) for image in images]\n",
        "    return pil_images\n",
        "  \n",
        "  # Find theta * del (L con)/ del (theta), and return, while setting new thetas\n",
        "  def process_attn(self,Attention, rho):\n",
        "    kv={}\n",
        "    kv['k']=(Attention.to_k.weight * (Attention.to_k.weight.grad)).detach()\n",
        "    kv['v']=(Attention.to_v.weight * (Attention.to_v.weight.grad)).detach()\n",
        "    Attention.to_k.weight=torch.nn.Parameter(Attention.to_k.weight- rho * Attention.to_k.weight * Attention.to_k.weight * Attention.to_k.weight.grad,requires_grad=True)\n",
        "    Attention.to_v.weight=torch.nn.Parameter(Attention.to_v.weight- rho * Attention.to_v.weight * Attention.to_v.weight * Attention.to_v.weight.grad,requires_grad=True)\n",
        "    return kv\n",
        "\n",
        "\n",
        "\n",
        "  def _encode_prompt(\n",
        "        self,\n",
        "        prompt,\n",
        "        device,\n",
        "        num_images_per_prompt,\n",
        "        do_classifier_free_guidance,\n",
        "        negative_prompt=None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    ):\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        if prompt_embeds is None:\n",
        "            text_inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            text_input_ids = text_inputs.input_ids\n",
        "            untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
        "                text_input_ids, untruncated_ids\n",
        "            ):\n",
        "                removed_text = self.tokenizer.batch_decode(\n",
        "                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n",
        "                )\n",
        "            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n",
        "                attention_mask = text_inputs.attention_mask.to(device)\n",
        "            else:\n",
        "                attention_mask = None\n",
        "\n",
        "            prompt_embeds = self.text_encoder(\n",
        "                text_input_ids.to(device),\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            prompt_embeds = prompt_embeds[0]\n",
        "\n",
        "        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n",
        "\n",
        "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
        "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
        "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
        "        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
        "            uncond_tokens: List[str]\n",
        "            if negative_prompt is None:\n",
        "                uncond_tokens = [\"\"] * batch_size\n",
        "            elif type(prompt) is not type(negative_prompt):\n",
        "                raise TypeError(\n",
        "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
        "                    f\" {type(prompt)}.\"\n",
        "                )\n",
        "            elif isinstance(negative_prompt, str):\n",
        "                uncond_tokens = [negative_prompt]\n",
        "            elif batch_size != len(negative_prompt):\n",
        "                raise ValueError(\n",
        "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
        "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
        "                    \" the batch size of `prompt`.\"\n",
        "                )\n",
        "            else:\n",
        "                uncond_tokens = negative_prompt\n",
        "\n",
        "            max_length = prompt_embeds.shape[1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                uncond_tokens,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n",
        "                attention_mask = uncond_input.attention_mask.to(device)\n",
        "            else:\n",
        "                attention_mask = None\n",
        "\n",
        "            negative_prompt_embeds = self.text_encoder(\n",
        "                uncond_input.input_ids.to(device),\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            negative_prompt_embeds = negative_prompt_embeds[0]\n",
        "\n",
        "        if do_classifier_free_guidance:\n",
        "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
        "            seq_len = negative_prompt_embeds.shape[1]\n",
        "\n",
        "            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n",
        "\n",
        "            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
        "            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
        "\n",
        "        return prompt_embeds\n",
        "\n",
        "\n",
        "  def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n",
        "    shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n",
        "    if isinstance(generator, list) and len(generator) != batch_size:\n",
        "        raise ValueError(\n",
        "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
        "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
        "        )\n",
        "\n",
        "    if latents is None:\n",
        "        latents = diffusers.utils.randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
        "    else:\n",
        "        latents = latents.to(device)\n",
        "\n",
        "    # scale the initial noise by the standard deviation required by the scheduler\n",
        "    latents = latents * self.scheduler.init_noise_sigma\n",
        "    return latents\n",
        "  \n",
        "  def prepare_extra_step_kwargs(self, generator, eta):\n",
        "    # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "    # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "    # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "    # and should be between [0, 1]\n",
        "\n",
        "    accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "    extra_step_kwargs = {}\n",
        "    if accepts_eta:\n",
        "        extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "    # check if the scheduler accepts generator\n",
        "    accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "    if accepts_generator:\n",
        "        extra_step_kwargs[\"generator\"] = generator\n",
        "    return extra_step_kwargs\n",
        "  \n",
        "  def decode_latents(self,latents):\n",
        "    latents = 1/self.vae.config.scaling_factor * latents\n",
        "    image=self.vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "    return image\n",
        "  \n",
        "  def __call__(\n",
        "        self,\n",
        "        prompt,\n",
        "        prompt_embeds: Optional[torch.FloatTensor]=None,\n",
        "        guidance_scale: int=7.5,\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int]  = None, \n",
        "        num_inference_steps: int = 50,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        output_type: str= 'pil',\n",
        "        return_dict: bool = True,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        eta: float = 0.0,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: int = 1,\n",
        "        init_latents: Optional[torch.FloatTensor] = None,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        ):\n",
        "    \n",
        "    if prompt is not None and isinstance(prompt, str):\n",
        "      batch_size = 1\n",
        "    elif prompt is not None and isinstance(prompt, list):\n",
        "      batch_size = len(prompt)\n",
        "    else:\n",
        "      batch_size = prompt_embeds.shape[0]\n",
        "    \n",
        "    device=self._execution_device()\n",
        "\n",
        "    do_classifier_free_guidance = guidance_scale > 1.0\n",
        "    prompt_embeds = self._encode_prompt(\n",
        "            prompt,\n",
        "            device,\n",
        "            1,\n",
        "            do_classifier_free_guidance,\n",
        "            negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "    \n",
        "    height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
        "    width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    #Prepare timesteps\n",
        "    self.scheduler.set_timesteps(num_inference_steps, device)\n",
        "    timesteps = self.scheduler.timesteps\n",
        "\n",
        "    #Prepare latents\n",
        "      #None in prepare_latents is to pass generator as None for prepare latents, for non-deterministic generation\n",
        "    num_channels_latents = self.unet.in_channels\n",
        "\n",
        "    latents = self.prepare_latents(\n",
        "       1 * 1,\n",
        "        num_channels_latents,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        init_latents,\n",
        "    )\n",
        "\n",
        "    num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "\n",
        "    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "    \n",
        "    with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                # expand the latents if we are doing classifier free guidance\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "\n",
        "                # predict the noise residual\n",
        "                noise_pred = self.unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    cross_attention_kwargs=cross_attention_kwargs,\n",
        "                ).sample\n",
        "\n",
        "\n",
        "                # perform guidance\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # compute the previous noisy sample x_t -> x_t-1\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "                # call the callback, if provided\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "    # 8. Post-processing\n",
        "    image = self.decode_latents(latents)\n",
        "\n",
        "    # 10. Convert to PIL\n",
        "    image = self.numpy_to_pil(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Compute and return masks in the order of masks for downsampling blocks, middle blocks, and upsampling blocks.\n",
        "  def cones_compute(self,train_data,steps, rho=2e-5,prior_loss_weight=1.0,activation_thresh=1e-3):\n",
        "    up_dict={}\n",
        "    down_dict={}\n",
        "    mid_dict={}\n",
        "    for step in range(steps):\n",
        "      torch.cuda.empty_cache()\n",
        "      for batch in train_data:\n",
        "        for module in self.unet.modules():\n",
        "          module.zero_grad()\n",
        "        with torch.no_grad():\n",
        "          latents = self.vae.encode(batch['pixel_values'].to(dtype=self.unet.dtype).to(self.unet.device)).latent_dist\n",
        "          latents = latents.sample()* self.vae.config.scaling_factor\n",
        "        # Get the text embedding for conditioning\n",
        "        with torch.no_grad():\n",
        "          encoder_hidden_states = self.text_encoder(batch[\"input_ids\"].to(self.text_encoder.device))[0]\n",
        "        # Sample noise that we'll add to the latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        bsz = latents.shape[0]\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "        timesteps = timesteps.long()\n",
        "        self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_k.state_dict()['weight'].requires_grad=True\n",
        "        # Add noise to the latents according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "\n",
        "  \n",
        "        # Predict the noise residual\n",
        "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "        # Get the target for loss depending on the prediction type\n",
        "        if self.scheduler.config.prediction_type == \"epsilon\":\n",
        "          target = noise\n",
        "        elif self.scheduler.config.prediction_type == \"v_prediction\":\n",
        "          target = self.scheduler.get_velocity(latents, noise, timesteps)\n",
        "        else:\n",
        "          raise ValueError(f\"Unknown prediction type {self.scheduler.config.prediction_type}\")\n",
        "\n",
        "        # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
        "        noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "        target, target_prior = torch.chunk(target, 2, dim=0)\n",
        "\n",
        "        # Compute instance loss\n",
        "        loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "        # Compute prior loss\n",
        "        prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
        "\n",
        "        # Add the prior loss to the instance loss.\n",
        "        loss = loss + prior_loss_weight * prior_loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Compute theta* del(L con )/ del theta, and new thetas for thetas in k,v layers in upsampling blocks\n",
        "        for i,block in enumerate(self.unet.up_blocks):\n",
        "          if (isinstance(block,CrossAttnUpBlock2D)):\n",
        "            for j in range(len(block.attentions)):\n",
        "              for k in block.attentions[j].transformer_blocks:\n",
        "                if str(i)+str(j)+'1' in up_dict:\n",
        "                  params_1=self.process_attn(k.attn1,rho)\n",
        "                  params_2=self.process_attn(k.attn2,rho)\n",
        "                  up_dict[str(i)+str(j)+'1']['k']+=params_1['k']\n",
        "                  up_dict[str(i)+str(j)+'1']['v']+=params_1['v']\n",
        "                  up_dict[str(i)+str(j)+'2']['k']+=params_2['k']\n",
        "                  up_dict[str(i)+str(j)+'2']['v']+=params_2['v']\n",
        "                else:\n",
        "                  up_dict[str(i)+str(j)+'1']=self.process_attn(k.attn1,rho)\n",
        "                  up_dict[str(i)+str(j)+'2']=self.process_attn(k.attn2,rho)\n",
        "\n",
        "        # Compute theta* del(L con )/ del theta, and new thetas for thetas in k,v layers in middle block\n",
        "        if '001' in mid_dict:\n",
        "          params_1=self.process_attn(self.unet.mid_block.attentions[0].transformer_blocks[0].attn1,rho)\n",
        "          params_2=self.process_attn(self.unet.mid_block.attentions[0].transformer_blocks[0].attn2,rho)\n",
        "          mid_dict['001']['k']+=params_1['k']\n",
        "          mid_dict['001']['v']+=params_1['v']\n",
        "          mid_dict['002']['k']+=params_2['k']\n",
        "          mid_dict['002']['v']+=params_2['v']\n",
        "\n",
        "        else:\n",
        "          mid_dict['001']=self.process_attn(self.unet.mid_block.attentions[0].transformer_blocks[0].attn1,rho)\n",
        "          mid_dict['002']=self.process_attn(self.unet.mid_block.attentions[0].transformer_blocks[0].attn2,rho)\n",
        "\n",
        "        \n",
        "        # Compute theta* del(L con )/ del theta, and new thetas for thetas in k,v layers in downsampling blocks\n",
        "        for i,block in enumerate(self.unet.down_blocks):\n",
        "          if (isinstance(block,CrossAttnDownBlock2D)):\n",
        "            for j in range(len(block.attentions)):\n",
        "              for k in block.attentions[j].transformer_blocks:\n",
        "                if str(i)+str(j)+'1' in down_dict:\n",
        "                  params_1=self.process_attn(k.attn1,rho)\n",
        "                  params_2=self.process_attn(k.attn2,rho)\n",
        "                  down_dict[str(i)+str(j)+'1']['k']+=params_1['k']\n",
        "                  down_dict[str(i)+str(j)+'1']['v']+=params_1['v']\n",
        "                  down_dict[str(i)+str(j)+'2']['k']+=params_2['k']\n",
        "                  down_dict[str(i)+str(j)+'2']['v']+=params_2['v']\n",
        "                else:\n",
        "                  down_dict[str(i)+str(j)+'1']=self.process_attn(k.attn1,rho)\n",
        "                  down_dict[str(i)+str(j)+'2']=self.process_attn(k.attn2,rho)\n",
        "\n",
        "    #Find masks for k v layers\n",
        "    for dicti in [down_dict, mid_dict, up_dict]:\n",
        "      for key in dicti:\n",
        "        for char in ['k','v']:\n",
        "          dicti[key][char]= activation_thresh * torch.ones_like(dicti[key][char])-dicti[key][char]\n",
        "          dicti[key][char]=torch.sign(F.relu(dicti[key][char]))\n",
        "    return down_dict,mid_dict,up_dict\n",
        "  \n",
        "\n",
        "  #Take masks and modify diffusion pipeline to generate concept images\n",
        "  def cones_inference(self,down_dict,mid_dict,up_dict, prompt, negative_prompt: Optional [Union[str, list[str]]]=None,num_inference_steps: int = 50, prompt_embeds:Optional[torch.FloatTensor] = None,height=512, width=512):\n",
        "    orig_down={}\n",
        "    orig_mid={}\n",
        "    orig_up={}\n",
        "    for i,block in enumerate(self.unet.down_blocks):\n",
        "      if (isinstance(block,CrossAttnDownBlock2D)):\n",
        "        for j in range(len(block.attentions)):\n",
        "          for k in block.attentions[j].transformer_blocks:\n",
        "            #attn1\n",
        "            orig_down[str(i)+str(j)+'1']={'k': k.attn1.to_k.weight, 'v': k.attn1.to_v.weight}\n",
        "            k.attn1.to_k.weight= nn.Parameter(k.attn1.to_k.weight * down_dict[str(i)+str(j)+'1']['k'],requires_grad=True)\n",
        "            k.attn1.to_v.weight= nn.Parameter(k.attn1.to_v.weight * down_dict[str(i)+str(j)+'1']['v'],requires_grad=True)\n",
        "\n",
        "            #attn2\n",
        "            orig_down[str(i)+str(j)+'1']={'k': k.attn2.to_k.weight, 'v': k.attn2.to_v.weight}\n",
        "            k.attn2.to_k.weight=nn.Parameter(k.attn2.to_k.weight * down_dict[str(i)+str(j)+'2']['k'],requires_grad=True)\n",
        "            k.attn2.to_v.weight=nn.Parameter(k.attn2.to_v.weight * down_dict[str(i)+str(j)+'2']['v'],requires_grad=True)\n",
        "\n",
        "    for i,block in enumerate(self.unet.up_blocks):\n",
        "      if (isinstance(block,CrossAttnUpBlock2D)):\n",
        "        for j in range(len(block.attentions)):\n",
        "          for k in block.attentions[j].transformer_blocks:\n",
        "            #attn1\n",
        "            orig_up[str(i)+str(j)+'1']={'k': k.attn1.to_k.weight, 'v': k.attn1.to_v.weight}\n",
        "            k.attn1.to_k.weight=nn.Parameter(k.attn1.to_k.weight * up_dict[str(i)+str(j)+'1']['k'],requires_grad=True)\n",
        "            k.attn1.to_v.weight=nn.Parameter(k.attn1.to_v.weight * up_dict[str(i)+str(j)+'1']['v'],requires_grad=True)\n",
        "\n",
        "            #attn2\n",
        "            orig_up[str(i)+str(j)+'1']={'k': k.attn2.to_k.weight, 'v': k.attn2.to_v.weight}\n",
        "            k.attn2.to_k.weight=nn.Parameter(k.attn2.to_k.weight * up_dict[str(i)+str(j)+'2']['k'],requires_grad=True)\n",
        "            k.attn2.to_v.weight=nn.Parameter(k.attn2.to_v.weight * up_dict[str(i)+str(j)+'2']['v'],requires_grad=True)\n",
        "\n",
        "    orig_mid['001']={'k': self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_k.weight , 'v': self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_v.weight }\n",
        "    orig_mid['002']={'k': self.unet.mid_block.attentions[0].transformer_blocks[0].attn2.to_k.weight , 'v': self.unet.mid_block.attentions[0].transformer_blocks[0].attn2.to_v.weight }\n",
        "    self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_k.weight=nn.Parameter(self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_k.weight* mid_dict['001']['k'],requires_grad=True)\n",
        "    self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_v.weight=nn.Parameter(self.unet.mid_block.attentions[0].transformer_blocks[0].attn1.to_v.weight* mid_dict['001']['v'],requires_grad=True)\n",
        "    self.unet.mid_block.attentions[0].transformer_blocks[0].attn2.to_k.weight=nn.Parameter(self.unet.mid_block.attentions[0].transformer_blocks[0].attn2.to_k.weight* mid_dict['002']['k'],requires_grad=True)\n",
        "    self.unet.mid_block.attentions[0].transformer_blocks[0].attn2.to_v.weight=nn.Parameter(self.unet.mid_block.attentions[0].transformer_blocks[0].attn2.to_v.weight* mid_dict['002']['v'],requires_grad=True)\n",
        "\n",
        "\n",
        "    img=self.__call__(prompt=prompt,negative_prompt=negative_prompt,num_inference_steps=num_inference_steps,prompt_embeds=prompt_embeds,height=height,width=width)\n",
        "    return img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9W-IE-mKF18"
      },
      "outputs": [],
      "source": [
        "cones=image_pipe('CompVis/stable-diffusion-v1-4',dtype=torch.float16, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqdWA1vPEwns"
      },
      "source": [
        "### Configure the below cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRjT9CleD9Yq"
      },
      "outputs": [],
      "source": [
        "#instance_prompt is for the concept to find neuron masks for\n",
        "#class prompt is for the class the concept belongs to, person, dog, cat, etc\n",
        "#instance_data_dir and class_data_dir are for paths to directories containing the concept images and the class images, respectively.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of <V*> dog\",\n",
        "        \"class_prompt\":         \"photo of a dog\",\n",
        "        \"instance_data_dir\":    \"/content/drive/MyDrive/subject_cones\",\n",
        "        \"class_data_dir\":       \"/content/drive/MyDrive/dog_imgs\"\n",
        "    }]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQRUEMPmEKnU"
      },
      "outputs": [],
      "source": [
        "train_dataset = DreamBoothDataset(\n",
        "    concepts_list=concepts_list,\n",
        "    tokenizer= cones.tokenizer, \n",
        "    with_prior_preservation=True,\n",
        "    size=512,\n",
        "    center_crop=False,\n",
        "    num_class_images=20,\n",
        "    hflip=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NuUP5NqFZzM"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "    input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "    pixel_values += [example[\"class_images\"] for example in examples]\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "    input_ids = cones.tokenizer.pad(\n",
        "        {\"input_ids\": input_ids},\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids\n",
        "\n",
        "    batch = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"pixel_values\": pixel_values,\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la47bINxuHV2",
        "outputId": "b6b14c6f-fbee-44ab-ff04-69b937e9de9c"
      },
      "outputs": [],
      "source": [
        "# steps is for passes through dataset, rho is learning rate, activation_thresh is the threshold for neuron masks\n",
        "#p,q,r are masks for down_blocks,mid_blocks, up_blocks respectively\n",
        "p,q,r=cones.cones_compute(train_dataloader,steps=100,rho=2e-5,activation_thresh=7e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzgn39TQ49nE"
      },
      "outputs": [],
      "source": [
        "del cones\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76B9XPTY4VZb"
      },
      "outputs": [],
      "source": [
        "cones=image_pipe('CompVis/stable-diffusion-v1-4',dtype=torch.float16, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d9622e8382d34ce0b09347df841a85b8",
            "cdea84898a9146a4acb859464f896ca8",
            "dac739d0626847b58833d38b2fdc3826",
            "7c0667ce79284cc2a06c3730fb236265",
            "75f713f5e71d4131b2c933371cd5add0",
            "fa973c677b71441ab3f3bb6fe0ac470c",
            "cab0740b4064440a94fb936ca2fec61b",
            "8db3f64d952f448e9222bf2226df8238",
            "9537339c6e1d42a48489925b04003ba2",
            "64f947d0091946ef9bd5be02b7d9347e",
            "195f134b23084822ac4e19de9efd3299"
          ]
        },
        "id": "JHAssmw9YEzY",
        "outputId": "4cecaf78-1e9c-4cf1-cb58-62770f9817db"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  img1=cones.cones_inference(p,q,r,'a photo of <V*> dog')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "8wdqWq5iajLp",
        "outputId": "678c9cc4-5921-4082-ab02-aef386a13f38"
      },
      "outputs": [],
      "source": [
        "img1[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "195f134b23084822ac4e19de9efd3299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64f947d0091946ef9bd5be02b7d9347e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f713f5e71d4131b2c933371cd5add0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c0667ce79284cc2a06c3730fb236265": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f947d0091946ef9bd5be02b7d9347e",
            "placeholder": "​",
            "style": "IPY_MODEL_195f134b23084822ac4e19de9efd3299",
            "value": " 50/50 [00:09&lt;00:00,  6.15it/s]"
          }
        },
        "8db3f64d952f448e9222bf2226df8238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9537339c6e1d42a48489925b04003ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cab0740b4064440a94fb936ca2fec61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdea84898a9146a4acb859464f896ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa973c677b71441ab3f3bb6fe0ac470c",
            "placeholder": "​",
            "style": "IPY_MODEL_cab0740b4064440a94fb936ca2fec61b",
            "value": "100%"
          }
        },
        "d9622e8382d34ce0b09347df841a85b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdea84898a9146a4acb859464f896ca8",
              "IPY_MODEL_dac739d0626847b58833d38b2fdc3826",
              "IPY_MODEL_7c0667ce79284cc2a06c3730fb236265"
            ],
            "layout": "IPY_MODEL_75f713f5e71d4131b2c933371cd5add0"
          }
        },
        "dac739d0626847b58833d38b2fdc3826": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db3f64d952f448e9222bf2226df8238",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9537339c6e1d42a48489925b04003ba2",
            "value": 50
          }
        },
        "fa973c677b71441ab3f3bb6fe0ac470c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
